[{"content":"AI Tools AI 助手 | ChatGPT AI 助手 | Kimi.ai 数据集检索 | OpenDataLab 数据集检索 | Google Dataset Search Prompt Engineering Guide | 提示工程指南 经典模型的 PyTorch 代码实现（带注释） AI 笔试面试题库 CCF-deadlines Previous PyTorch Versions | PyTorch（本人常用😸） Basic Tools 论文检索 | dblp 速查工具 | Quick Reference 论文检索 | Papers With Code 流程图绘制 | ProcessOn 手绘风格白板工具 | ExcaliDraw 在线 LaTeX 公式编辑器-编辑器 免费的 PDF 工具 | PDF24 Tools E-Book | Z-library（其链接经常变动，检索 Z-library 即可） Git 代码下载 | DownGit 如何给 GitHub 上的项目贡献代码 ","permalink":"https://ShoutaoSun.github.io/posts/tools/","summary":"Some AI tools that I think are useful✍","title":"Practical Tools of AI"},{"content":"前言 提示工程 (Prompt Engineering) 是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型 (Large Language Model, LLM) 用于各场景和研究领域。掌握了提示工程相关技能将有助于用户更好地了解大型语言模型的能力和局限性。\n研究人员可利用提示工程来提升大语言模型处理复杂任务场景的能力，如问答和算术推理能力。开发人员可通过提示工程设计、研发强大的工程技术，实现和大语言模型或其他生态工具的高效接轨。\n提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性，也可以赋能大语言模型，比如借助专业领域知识和外部工具来增强大语言模型能力。\n提示工程简介 本部分介绍了提示词相关的基础知识，帮助了解如何通过提示词和 LLM 进行交互并提供指导建议。\n模型设置 使用提示词时，我们通常会使用 API 或直接与 LLM 进行交互。我们可以通过配置一些参数以获得不同的提示结果。调整这些设置对于提高响应的可靠性非常重要，需要进行一些实验才能找出适合个人用例的正确设置。以下是使用不同 LLM 提供程序时会遇到的常见设置。\nTemperature 用于控制模型返回结果的确定性。temperature的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，模型就会返回越确定的一个结果。如果调高该参数值，LLM 会返回更随机的结果。调小temperature实际上是在增加其他可能的 token 的权重。\n实际应用：\n质量保障 (QA)：设置更低的temperature，促使模型基于事实返回更真实和简洁的结果 诗歌生成：适度地调高temperature参数值可能会更好 Top_p 用于控制模型返回结果的确定性。与temperature类似，如果需要准确和事实的答案，就把参数值调低；如果需要更多样化的响应，就把参数值调高。\n一般建议是改变temperature和top_p中的一个参数即可，不需要两个都调整。\nMax Length 用于控制大模型生成的 token 数。指定Max Length有助于防止大模型生成冗长或不相关的响应，同时控制成本。\nStop Sequences stop sequence是一个字符串，可以阻止模型生成 token，指定stop sequence是控制 LLM 响应长度和结构的另一种方法。eg: 通过添加“11”作为stop sequence来告诉模型生成不超过 10 个项的列表。\nFrequency Penalty frequency penalty是对下一个生成的 token 进行惩罚，这个惩罚和 token 在响应和提示中已出现的次数成比例，frequency penalty越高，某个词再次出现的可能性就越小，通过设置frequency penalty可以给重复数量多的 token 设置更高的惩罚以减少响应中单词的重复。\nPresence Penalty presence penalty也是对重复的 token 施加惩罚，但与frequency penalty不同，presence penalty对于所有重复的 token 都是相同的（出现 2 次的 token 和出现 10 次的 token 会受到相同的惩罚）。此设置可以防止模型在响应中过于频繁地生成重复的词。\n与temperature和top_p一样，一般建议是改变frequency penalty和presence penalty其中一个参数就行，无须同时调整两个。\n基本概念 设计有效的提示词以指导模型执行期望任务的方法被称为提示工程。\n提示词格式 标准提示词应遵循以下格式。\n\u0026lt;问题\u0026gt;? 或\n\u0026lt;指令\u0026gt;? 提示词要素 提示词可包含以下任意要素。\n指令：想要模型执行的特定任务或指令； 上下文：包含外部信息或额外的上下文信息，引导语言模型更好地响应； 输入数据：用户输入的内容或问题； 输出指示：指定输出的类型或格式。 示例如下：\n请将文本分为肯定、否定或中性。 # 指令 文本：我觉得食物还可以。 # 输入数据 情绪： # 输出指示 补充说明，提示词所需的格式取决于用户想要语言模型完成的任务类型，并非所有以上要素都是必须的。\n设计提示的通用技巧 指令：使用命令指示模型执行各种简单任务，例如“写入”、“分类”、“总结”等，从而为各种简单任务设计有效的提示；需要以不同的关键词、上下文和数据对不同的指令进行大量实验，寻找最适合特定用例和任务的指令 具体性：具有良好格式和描述性的提示词，同时注意提示的长度，不要包含太多不必要的细节 避免不准确：具体、简洁并切中要点，例如：“使用 2-3 句话向高中学生解释提示工程的概念” 做什么还是不做什么：避免说不要做什么，而应该说要做什么 更多的参考案例详见 Best practices for prompt engineering with the OpenAI API\n提示技术 零样本提示 直接提示模型给出一个回答，而没有提供任何关于你希望它完成的任务的示例。案例如下所示。\n提示：\n将文本分类为中性、负面或正面。 文本：我认为这次假期还可以。 情感： 输出：\n中性 指令调整已被证明可以改善零样本学习 Wei et al (2022) 。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，RLHF 已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。\n","permalink":"https://ShoutaoSun.github.io/posts/prompt_engineering/","summary":"Some basic knowledge you must know during the process of Prompt Engineering learning💻","title":"The Basic Knowledge of Prompt Engineering"},{"content":"Abstract $a+b$\nKeywords ","permalink":"https://ShoutaoSun.github.io/posts/adv_llm_11_17/","summary":"A survey of adversarial attacks and defenses for LLM","title":"Adversarial attacks and defenses for large language models (LLMs): methods, frameworks \u0026 challenges"},{"content":"Basic Information 本文是 University of Würzburg 的Torsten Krauß, Jasper Stang, Alexandra Dmitrienko 发表在 2024 年的 USENIX Security (安全四大会)的一篇文章，文章关注的内容是数据清洗、置信度评分和投毒检测。\n原文链接：Verify your Labels! Trustworthy Predictions and Datasets via Confidence Scores | USENIX\n【注】本文未将代码开源。\nResearch Question ","permalink":"https://ShoutaoSun.github.io/posts/labeltrust/","summary":"A tool that verifies the trustworthiness of sample-label mappings in machine learning","title":"Verify your Labels! Trustworthy Predictions and Datasets via Confidence Scores"}]